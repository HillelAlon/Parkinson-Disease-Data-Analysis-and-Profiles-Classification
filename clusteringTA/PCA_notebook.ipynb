{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Setup and Logging",
   "id": "6cca96ba58fe4edb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Professional logging setup as per project requirements\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Environment setup complete. Libraries imported.\")"
   ],
   "id": "dc42367ab9842d9a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#  Data Loading\n",
   "id": "7c3c443eaeab0e17"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Loading the pre-processed dataset specifically prepared for PCA\n",
    "try:\n",
    "    df_pca = pd.read_csv(\"../data/parkinsons_lifestyle_clinical_for_PCA.csv\", index_col=0)\n",
    "    logger.info(f\"Dataset loaded successfully. Shape: {df_pca.shape}\")\n",
    "except FileNotFoundError:\n",
    "    logger.error(\"Dataset file not found. Please check the file path.\")"
   ],
   "id": "c6fd596557a85264",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Feature Scaling (Standardization)",
   "id": "ae04244f5956ef6f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# PCA is scale-sensitive, so we must transform features to a common scale (Z-scores)\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df_pca)\n",
    "\n",
    "logger.info(\"Feature scaling complete. Data is now normalized.\")\n",
    "print(scaled_data)\n",
    "print(scaled_data.shape)"
   ],
   "id": "40a7d5dad50fce61",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#  Dimensionality Reduction (PCA)",
   "id": "2555e2fd9b724fff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Define the PCA model to reduce data into 3 components\n",
    "pca = PCA(n_components=3)\n",
    "\n",
    "# 2. Run the PCA on our standardized data\n",
    "pca_results = pca.fit_transform(scaled_data)\n",
    "\n",
    "# 3. Create a new DataFrame for organized results and easy visualization\n",
    "df_pca_output = pd.DataFrame(\n",
    "    data=pca_results,\n",
    "    columns=['PC1', 'PC2', 'PC3'],\n",
    "    index=df_pca.index  # Keeping the original Patient IDs as index\n",
    ")\n",
    "print(df_pca_output)\n",
    "# Professional logging of the process\n",
    "logger.info(f\"PCA execution finished. Features reduced from {df_pca.shape[1]} to 3 components.\")"
   ],
   "id": "e1846d17b3607107",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Explained Variance Analysis (Validation)",
   "id": "38cabea9687e3f9f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Get the percentage of variance explained by each of the 3 components\n",
    "variance_ratios = pca.explained_variance_ratio_\n",
    "total_variance = np.sum(variance_ratios)\n",
    "\n",
    "# 2. Log the results for documentation\n",
    "logger.info(f\"Variance explained by PC1: {variance_ratios[0]:.2%}\")\n",
    "logger.info(f\"Variance explained by PC2: {variance_ratios[1]:.2%}\")\n",
    "logger.info(f\"Variance explained by PC3: {variance_ratios[2]:.2%}\")\n",
    "logger.info(f\"Total variance captured by all 3 components: {total_variance:.2%}\")\n",
    "\n",
    "# 3. Simple validation check\n",
    "if total_variance > 0.70:\n",
    "    logger.info(\"Validation Success: More than 70% of the information was retained.\")\n",
    "else:\n",
    "    logger.warning(\"Validation Note: Captured variance is below 70%. We might need to consider more components later.\")"
   ],
   "id": "468eaa566d981440",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check how many components are needed to reach 70% variance\n",
    "full_pca = PCA().fit(scaled_data)\n",
    "cumulative_variance = np.cumsum(full_pca.explained_variance_ratio_)\n",
    "\n",
    "# Finding the number of components for 70% threshold\n",
    "n_70 = np.where(cumulative_variance >= 0.70)[0][0] + 1\n",
    "\n",
    "logger.info(f\"To explain 70% of the variance, we would need {n_70} components.\")\n",
    "\n",
    "# Plotting the \"Scree Plot\"\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--')\n",
    "plt.axhline(y=0.7, color='r', linestyle='-')\n",
    "plt.title('How many components do we actually need?')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "50f0e1500cc88884",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Scree Plot & Cumulative Variance\n",
    "Due to the low correlation between clinical and lifestyle variables (as seen in the linear cumulative variance plot), we selected the first 3 components to capture the most dominant trends while maintaining interpretability."
   ],
   "id": "55c5f3c7396557a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Run PCA without limiting the number of components to see the full picture\n",
    "full_pca = PCA().fit(scaled_data)\n",
    "cumulative_variance = np.cumsum(full_pca.explained_variance_ratio_)\n",
    "\n",
    "# 2. Find exactly how many components are needed for 70% threshold\n",
    "n_70 = np.where(cumulative_variance >= 0.70)[0][0] + 1\n",
    "logger.info(f\"To explain 80% of the variance, we would need {n_70} components.\")\n",
    "\n",
    "# 3. Visualization: The Scree Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--', color='b')\n",
    "plt.axhline(y=0.7, color='r', linestyle='-', label='70% Threshold')\n",
    "plt.axhline(y=total_variance, color='g', linestyle='--', label='Current 3 Components')\n",
    "\n",
    "plt.title('Scree Plot: How much information are we capturing?')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ],
   "id": "abf7bedaa35d0ba3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " # 3D Visualization of Patient Profiles",
   "id": "6981c75fbf4d5919"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Create a 3D figure\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# 2. Scatter plot\n",
    "# c=df_pca_output['PC1'] means it will color the points by their PC1 value (looks professional)\n",
    "sc = ax.scatter(df_pca_output['PC1'],\n",
    "                df_pca_output['PC2'],\n",
    "                df_pca_output['PC3'],\n",
    "                c=df_pca_output['PC1'],\n",
    "                cmap='viridis',\n",
    "                s=40,\n",
    "                alpha=0.6)\n",
    "\n",
    "# 3. Labels and Title\n",
    "ax.set_title(\"3D PCA: Parkinson's Patient Profiles\", fontsize=15)\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "\n",
    "# 4. Add a color bar\n",
    "plt.colorbar(sc, label='PC1 Gradient')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "logger.info(\"3D Visualization created using Matplotlib.\")"
   ],
   "id": "5f4c6c05d888dc37",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# The Elbow Method",
   "id": "1f7c03b302f11fe2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# 1. Calculate inertia for different numbers of clusters\n",
    "inertia = []\n",
    "K_range = range(1, 11) # Checking from 1 to 10 clusters\n",
    "\n",
    "for k in K_range:\n",
    "    km = KMeans(n_clusters=k, random_state=42)\n",
    "    km.fit(pca_results)\n",
    "    inertia.append(km.inertia_)\n",
    "\n",
    "# 2. Plot the Elbow graph\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(K_range, inertia, marker='o', linestyle='-', color='purple')\n",
    "plt.title('The Elbow Method: Finding Optimal Clusters', fontsize=15)\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia (Error)')\n",
    "plt.xticks(K_range)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Highlight the \"Elbow\"\n",
    "plt.show()\n",
    "\n",
    "logger.info(\"Elbow Method analysis completed.\")"
   ],
   "id": "9e4b6f57c8ea567a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# K-Means Clustering",
   "id": "97eafe9844647d4f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# 1. Initialize the KMeans model\n",
    "# We choose 4 clusters to look for 4 distinct patient profiles\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "\n",
    "# 2. Fit the model using our PCA results\n",
    "df_pca_output['Cluster'] = kmeans.fit_predict(pca_results)\n",
    "\n",
    "# 3. Log the success\n",
    "logger.info(\"Clustering complete. Patients have been assigned to 4 distinct profiles.\")\n",
    "\n",
    "# 4. Let's see how many patients are in each cluster\n",
    "print(df_pca_output['Cluster'].value_counts())"
   ],
   "id": "a1f34b0395dc1fda",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Visualizing the Clusters in 3D",
   "id": "3b52f3474e2807c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cluster Profiling",
   "id": "a0b99ad02c0f281d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Add the Cluster labels back to the ORIGINAL dataframe (the one with real units)\n",
    "df_pca['Cluster'] = df_pca_output['Cluster']\n",
    "\n",
    "# 2. Calculate the mean (average) for each variable per cluster\n",
    "cluster_profiles = df_pca.groupby('Cluster').mean()\n",
    "\n",
    "# 3. Display the results\n",
    "print(\"Profiles for each Cluster (Mean Values):\")\n",
    "display(cluster_profiles)\n",
    "\n",
    "# 4. Save the results to a CSV file to use in your presentation slides\n",
    "\n",
    "cluster_profiles.to_csv(\"patient_profiles_summary.csv\")\n",
    "logger.info(\"Cluster profiling complete. Summary saved to CSV.\")"
   ],
   "id": "c152aeaf54a2fcf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Creation of a Heat map for clusters\n",
    "We normalized the mean values across clusters using Z-score standardization to allow for a direct comparison. In this heatmap, a value of 0 represents the overall population mean. Red cells indicate values significantly above the average, while blue cells represent values below the average."
   ],
   "id": "64b42af868003185"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "#Heat map preparing : gives a Z-Score for every cluster_profile-feature-mean\n",
    "df_pca_original = df_pca.drop(columns=['Cluster'])\n",
    "cluster_profiles_norm = (cluster_profiles - df_pca_original.mean()) / df_pca_original.std()\n",
    "\n",
    "# Creating Heat map\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(cluster_profiles_norm,annot=cluster_profiles, cmap='RdYlBu_r', center=0, fmt = '.2f')\n",
    "\n",
    "plt.title(\"Patient Profiles Characteristics (Heat map)\", fontsize=16)\n",
    "plt.ylabel(\"Cluster ID\")\n",
    "plt.xlabel(\"Medical & Lifestyle Features\")\n",
    "plt.show()\n",
    "\n",
    "logger.info(\"Visual profile summary created.\")"
   ],
   "id": "3df61c95836f8a64",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Differences between Clusters per Assessment\n",
    "We choose to make an one-way ANOVA test for Clusters per Assessment. After this, we pick to do 'Pairwise comparisons': Tests all possible pairs."
   ],
   "id": "4af7ce2dff2bf06d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T20:43:57.485429Z",
     "start_time": "2026-01-14T20:43:57.461998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from scipy.stats import f_oneway\n",
    "from scipy.stats import ttest_ind\n",
    "from math import comb\n",
    "df = pd.read_csv(\"../data/parkinsons_cleaned.csv\")\n",
    "\n",
    "# Keep only the sick patients in the data frame.\n",
    "df = df[df['Diagnosis'] != 0]\n",
    "\n",
    "# Keep only age, lifestyle and clinical measurements categories\n",
    "bool_cols_to_drop = [col for col in df.columns if len(set(df[col].unique())) < 5] #Remove boolean and categorical columns\n",
    "df = df.drop(columns=bool_cols_to_drop)\n",
    "df = df.drop(columns=[\"PatientID\"]) #Remove ID\n",
    "\n",
    "# Here We are doing the statistical test\n",
    "our_p_value = 0.20\n",
    "print(f\"One-way analysis of variance\\nH0: Samples in all groups are drawn from populations with the same mean values.\\n Our critical P-value (alpha): {our_p_value}\\n(We teke this p_value because the data is synthetic...)\\n\")\n",
    "\n",
    "for assessment in [\"UPDRS\", \"MoCA\", \"FunctionalAssessment\"]:\n",
    "    df_corr = pd.concat([df_pca['Cluster'], df[assessment]], axis=1)\n",
    "    f_statistic, p_value = f_oneway(df_corr[df_corr['Cluster'] == 0][assessment],df_corr[df_corr['Cluster'] == 1][assessment],df_corr[df_corr['Cluster'] == 2][assessment],df_corr[df_corr['Cluster'] == 3][assessment])\n",
    "    print(f\"{assessment}\\nF-statistic: {round(f_statistic,3)}\\nP-value: {round(p_value,3)}\")\n",
    "    if p_value >= our_p_value:\n",
    "        print(f\"Not significant, we cannot reject the null hypothesis.\\n\")\n",
    "    else:\n",
    "        print(f\"Significant! We can reject the null hypothesis.\")\n",
    "        print(f\"We need to do a Post hoc analysis.\\nWe pick to do 'Pairwise comparisons': Tests all possible pairs.\\nDon't forget to counteract the multiple comparisons problem.\\nSo we are doing the 'Bonferroni correction'\")\n",
    "        our_new_p_value = our_p_value/comb(len(df_pca['Cluster'].unique()),2)\n",
    "        print(f\"Our new critical P-value (alpha): {round(our_new_p_value,3)}\")\n",
    "        our_new_p_value = our_new_p_value/2\n",
    "        print(f\"We need to divide this by two because the test is two-tailed.\")\n",
    "        print(f\"Our new critical P-value (alpha): {round(our_new_p_value,3)}\")\n",
    "        couple_of_clusters = [[0,1],[0,2],[0,3],[1,2],[1,3],[2,3]]\n",
    "        for couple in couple_of_clusters:\n",
    "            t_stat, t_p_value = ttest_ind(df_corr[df_corr['Cluster'] == couple[0]][assessment],df_corr[df_corr['Cluster'] == couple[1]][assessment])\n",
    "            print(f\"{couple[0]},{couple[1]}: {round(t_p_value,3)}\")\n",
    "            if t_p_value >= our_new_p_value:\n",
    "                print(f\"Not significant.\")\n",
    "            else:\n",
    "                print(f\"Significant!\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "logger.info(\"One way analysis of variance complete.\")"
   ],
   "id": "8dc8380bdaf9c561",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-14 22:43:57,484 - INFO - One way analysis of variance complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-way analysis of variance\n",
      "H0: Samples in all groups are drawn from populations with the same mean values.\n",
      " Our critical P-value (alpha): 0.2\n",
      "(We teke this p_value because the data is synthetic...)\n",
      "\n",
      "UPDRS\n",
      "F-statistic: 1.404\n",
      "P-value: 0.24\n",
      "Not significant, we cannot reject the null hypothesis.\n",
      "\n",
      "MoCA\n",
      "F-statistic: 1.094\n",
      "P-value: 0.351\n",
      "Not significant, we cannot reject the null hypothesis.\n",
      "\n",
      "FunctionalAssessment\n",
      "F-statistic: 1.749\n",
      "P-value: 0.155\n",
      "Significant! We can reject the null hypothesis.\n",
      "We need to do a Post hoc analysis.\n",
      "We pick to do 'Pairwise comparisons': Tests all possible pairs.\n",
      "Don't forget to counteract the multiple comparisons problem.\n",
      "So we are doing the 'Bonferroni correction'\n",
      "Our new critical P-value (alpha): 0.033\n",
      "We need to divide this by two because the test is two-tailed.\n",
      "Our new critical P-value (alpha): 0.017\n",
      "0,1: 0.789\n",
      "Not significant.\n",
      "0,2: 0.125\n",
      "Not significant.\n",
      "0,3: 0.6\n",
      "Not significant.\n",
      "1,2: 0.07\n",
      "Not significant.\n",
      "1,3: 0.799\n",
      "Not significant.\n",
      "2,3: 0.037\n",
      "Not significant.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 161
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
